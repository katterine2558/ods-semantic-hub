{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/katterine2558/ods-semantic-hub/blob/main/ods_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SOLUCIÓN MICROPROYECTO 2\n",
        "\n",
        "Por: *Leonardo Almanza y Katerine Arias*"
      ],
      "metadata": {
        "id": "EfAC-CLZ0So1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Importar librerías\n",
        "\n",
        "En esta etapa inicial se realiza la importación de las librerías esenciales para el desarrollo de una solución automatizada de clasificación de textos orientada a los 17 Objetivos de Desarrollo Sostenible (ODS). Se incorporan herramientas clave para la manipulación y análisis de datos (Pandas, NumPy), el procesamiento de lenguaje natural (NLTK, con word_tokenize, stopwords y PorterStemmer), la extracción de características textuales (CountVectorizer, TfidfVectorizer), la implementación de algoritmos de aprendizaje supervisado (Regresión Logística) y la aplicación de técnicas de reducción de dimensionalidad (PCA, TruncatedSVD). Estas bibliotecas permiten transformar los textos en representaciones vectoriales bajo el esquema de bolsa de palabras (BOW) con ponderación TF-IDF, aplicar métodos de reducción para controlar la alta dimensionalidad del espacio de entrada y, finalmente, construir modelos capaces de realizar clasificación automática de manera eficiente."
      ],
      "metadata": {
        "id": "UNXWLN8717GW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K0PZ5vaOWFGg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "\n",
        "# NLTK para procesamiento de texto\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "import nltk\n",
        "\n",
        "# Scikit-learn\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.decomposition import PCA, TruncatedSVD\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    nltk.download('punkt')\n",
        "    nltk.download('stopwords')\n",
        "    nltk.download('punkt_tab')\n",
        "    print(\"Recursos NLTK descargados exitosamente\")\n",
        "except:\n",
        "    print(\"Algunos recursos de NLTK no se pudieron descargar\")\n",
        "\n",
        "print(\"Librerías importadas exitosamente\")"
      ],
      "metadata": {
        "id": "5F_exeEnVtTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Análisis exploratorio de datos\n",
        "\n",
        "En esta etapa se realiza la exploración inicial del conjunto de datos, cargando la información desde un archivo Excel y verificando su estructura, columnas disponibles y distribución de las clases asociadas a los Objetivos de Desarrollo Sostenible (ODS). Posteriormente, se lleva a cabo un análisis descriptivo que incluye la revisión de valores nulos, la generación de estadísticas básicas de los textos (longitud en caracteres y número de palabras) y la visualización de la distribución de las clases mediante gráficos de barras y diagramas de pastel. Adicionalmente, se analiza la variabilidad en el tamaño de los textos a través de histogramas, lo cual permite obtener una visión preliminar sobre la calidad, balance y características principales del dataset antes de aplicar técnicas de preprocesamiento y modelado."
      ],
      "metadata": {
        "id": "6vTo5f-JS0Uh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2.1. Información del dataset\n",
        "\n",
        "El conjunto de datos cuenta con 9.656 registros distribuidos en dos columnas: textos, que contiene los fragmentos a analizar, y ODS, que representa la categoría correspondiente al Objetivo de Desarrollo Sostenible. Adicionalmente se verifica que no hay valores nulos y tampoco hay duplicados."
      ],
      "metadata": {
        "id": "oFisu-iTc-4h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar datos\n",
        "df = pd.read_excel('data.xlsx')\n",
        "\n",
        "#Se crea copia sobre el dataset original\n",
        "data_raw = df.copy()\n",
        "\n",
        "print(\"=== INFORMACIÓN DEL DATASET ===\")\n",
        "print(f\"Tamaño del dataset: {df.shape}\")\n",
        "print(f\"Columnas: {list(df.columns)}\")\n",
        "print(\"\\nPrimeras 5 filas:\")\n",
        "print(df.head())\n",
        "\n",
        "# Verificar valores nulos\n",
        "print(f\"\\nValores nulos:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "#Verificar duplicados\n",
        "print(f\"\\nDuplicados:\")\n",
        "print(df.duplicated().sum())\n",
        "\n"
      ],
      "metadata": {
        "id": "BiMx78eGdDhS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2. Distribución de la variable target\n",
        "\n",
        "La distribución de los textos por ODS evidencia un desbalance en las clases, con mayor concentración en los ODS 4, 5 y 16, mientras que otros presentan una representación considerablemente menor. Además, aunque el enunciado menciona 17 ODS, el dataset solo contiene 16 categorías, por lo que la clasificación se realizará bajo estos 16 objetivos."
      ],
      "metadata": {
        "id": "ApTKaHpKdraO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\nDistribución de clases (ODS):\")\n",
        "ods_counts = df['ODS'].value_counts().sort_index()\n",
        "print(ods_counts)\n",
        "\n",
        "# Visualización básica con matplotlib\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "ods_counts.plot(kind='bar', color='skyblue', alpha=0.7)\n",
        "plt.title('Distribución de Textos por ODS')\n",
        "plt.xlabel('Objetivo de Desarrollo Sostenible')\n",
        "plt.ylabel('Número de Textos')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.pie(ods_counts.values, labels=ods_counts.index, autopct='%1.1f%%', startangle=90)\n",
        "plt.title('Distribución Porcentual por ODS')"
      ],
      "metadata": {
        "id": "7SfZkqCyd1a3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3. Otros análisis\n",
        "\n",
        "El análisis de la longitud de los textos muestra que, en promedio, cada registro contiene alrededor de 709 caracteres y 111 palabras. El texto más corto cuenta con 143 caracteres, mientras que el más extenso alcanza los 1977. La distribución del número de palabras presenta una forma cercana a una campana, concentrando la mayoría de los textos entre 70 y 130 palabras. Esta información es relevante, ya que permite dimensionar el tamaño típico de los textos y anticipar la complejidad del procesamiento posterior."
      ],
      "metadata": {
        "id": "IVrnEULJfALk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Estadísticas básicas de texto\n",
        "df['longitud_texto'] = df['textos'].str.len()\n",
        "df['num_palabras'] = df['textos'].str.split().str.len()\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.hist(df['num_palabras'], bins=30, alpha=0.7, color='lightgreen')\n",
        "plt.title('Distribución de Número de Palabras')\n",
        "plt.xlabel('Número de Palabras')\n",
        "plt.ylabel('Frecuencia')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\n=== ESTADÍSTICAS DE TEXTO ===\")\n",
        "print(f\"Longitud promedio de caracteres: {df['longitud_texto'].mean():.1f}\")\n",
        "print(f\"Longitud promedio de palabras: {df['num_palabras'].mean():.1f}\")\n",
        "print(f\"Texto más corto: {df['longitud_texto'].min()} caracteres\")\n",
        "print(f\"Texto más largo: {df['longitud_texto'].max()} caracteres\")"
      ],
      "metadata": {
        "id": "Uhehno_-9BGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Preprocesamiento de textos\n",
        "\n",
        "El preprocesamiento de textos constituye una etapa fundamental para garantizar la calidad de los datos antes de su vectorización y posterior modelado:\n",
        "\n",
        "1. **Limpieza**: el texto es normalizado para reducir ruido y homogenizar la información.\n",
        "2. **Tokenización**: divide el texto en unidades mínimas de análisis (palabras o tokens).\n",
        "3. **Eliminación de stopwords**: elimina aquellas palabras sin valor semántico relevante para la clasificación.\n",
        "4. **Stemming**: técnica que reduce las palabras a sus raíces para agrupar variantes morfológicas y disminuir la dimensionalidad del vocabulario.\n"
      ],
      "metadata": {
        "id": "NfzCk82JSoYt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1. Procesador de texto\n",
        "\n",
        "Se implementa un transformador personalizado compatible con pipelines de scikit-learn que normaliza el texto, elimina caracteres especiales, tokeniza, remueve stopwords y aplica stemming, generando una versión depurada y lista para la vectorización."
      ],
      "metadata": {
        "id": "wvADJld2mQCh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "STOP_WORDS = set(stopwords.words(\"spanish\"))\n",
        "\n",
        "class TextPreprocessor(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Transformador personalizado para preprocesamiento de texto\n",
        "    Integrable en pipelines de sklearn\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language=\"spanish\"):\n",
        "        self.language = language\n",
        "        self.stop_words = STOP_WORDS\n",
        "        self.stemmer = PorterStemmer()\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        # No hay parámetros que ajustar\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"\n",
        "        Aplica preprocesamiento completo a una serie de textos\n",
        "        \"\"\"\n",
        "        return X.apply(self._preprocess_text)\n",
        "\n",
        "    def _preprocess_text(self, text):\n",
        "        \"\"\"\n",
        "        Función de preprocesamiento integrada en el transformador\n",
        "        \"\"\"\n",
        "        if pd.isna(text):\n",
        "            return \"\"\n",
        "\n",
        "        # Convertir a minúsculas\n",
        "        text = text.lower()\n",
        "\n",
        "        # Eliminar caracteres especiales, conservar solo letras y espacios\n",
        "        text = re.sub(r'[^a-záéíóúñü\\s]', ' ', text)\n",
        "\n",
        "        # Eliminar espacios múltiples\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "        # Tokenización\n",
        "        try:\n",
        "            tokens = word_tokenize(text, language=self.language)\n",
        "        except:\n",
        "            tokens = word_tokenize(text)\n",
        "\n",
        "        # Eliminar stopwords, palabras cortas y aplicar stemming\n",
        "        tokens = [self.stemmer.stem(word) for word in tokens\n",
        "                  if word not in self.stop_words and len(word) > 2]\n",
        "\n",
        "        return ' '.join(tokens)\n"
      ],
      "metadata": {
        "id": "6topGzSVZUb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se crea el objeto del preprocesador:"
      ],
      "metadata": {
        "id": "3eptpiEFhKy8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_preprocessor = TextPreprocessor(language='spanish')"
      ],
      "metadata": {
        "id": "b4Z1KM5CgY3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2. Test de transformador\n",
        "\n",
        "Se probó el transformador en una muestra de textos para verificar su funcionamiento. Los resultados muestran cómo los textos originales fueron normalizados, tokenizados, depurados de stopwords y reducidos mediante stemming."
      ],
      "metadata": {
        "id": "XS8opISgiDMn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== EJEMPLOS DE PREPROCESAMIENTO CON TRANSFORMADOR ===\")\n",
        "\n",
        "# Seleccionar algunos textos para demostrar\n",
        "textos_ejemplo = df['textos'].head(3)\n",
        "\n",
        "print(\"Aplicando transformador a textos de ejemplo...\")\n",
        "textos_procesados = text_preprocessor.transform(textos_ejemplo)\n",
        "for i in range(len(textos_ejemplo)):\n",
        "    print(f\"\\nEjemplo {i+1}:\")\n",
        "    print(f\"Original: {textos_ejemplo.iloc[i][:200]}...\")\n",
        "    print(f\"Procesado: {textos_procesados.iloc[i][:200]}...\")"
      ],
      "metadata": {
        "id": "A0y0rAonkCEP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3. Análisis del transformador\n",
        "\n",
        "El análisis comparativo entre los textos originales y procesados muestra una reducción significativa en su longitud. En promedio, la cantidad de caracteres disminuyó a 472.9 y el número de palabras a 56.5, lo que representa una reducción global del 33.3%. Los histogramas confirman esta tendencia, evidenciando cómo el preprocesamiento depura el contenido eliminando ruido y manteniendo únicamente la información más relevante para el modelado.\n"
      ],
      "metadata": {
        "id": "i0hdPknfmy3s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Estadísticas del preprocesamiento\n",
        "# Métricas originales\n",
        "df['longitud_texto'] = df['textos'].str.len()\n",
        "df['num_palabras'] = df['textos'].str.split().str.len()\n",
        "\n",
        "# Aplica preprocesador y asegúrate de que quede como string\n",
        "df['texto_procesado_transformer'] = text_preprocessor.transform(df[\"textos\"]).apply(lambda x: \" \".join(x) if isinstance(x, list) else str(x))\n",
        "\n",
        "# Métricas procesadas\n",
        "df['longitud_procesado'] = df['texto_procesado_transformer'].str.len()\n",
        "df['num_palabras_procesado'] = df['texto_procesado_transformer'].str.split().str.len()\n",
        "\n",
        "# Estadísticas generales\n",
        "print(f\"\\n=== ESTADÍSTICAS DEL PREPROCESAMIENTO ===\")\n",
        "print(f\"Longitud promedio después de procesamiento: {df['longitud_procesado'].mean():.1f} caracteres\")\n",
        "print(f\"Palabras promedio después de procesamiento: {df['num_palabras_procesado'].mean():.1f}\")\n",
        "print(f\"Reducción de longitud: {((df['longitud_texto'].mean() - df['longitud_procesado'].mean()) / df['longitud_texto'].mean() * 100):.1f}%\")\n",
        "\n",
        "# Comparación visual\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.hist(df['longitud_texto'].dropna(), bins=50, alpha=0.7, color='lightblue', label='Original')\n",
        "plt.hist(df['longitud_procesado'].dropna(), bins=50, alpha=0.7, color='lightcoral', label='Procesado')\n",
        "plt.title('Distribución de Longitud de Textos')\n",
        "plt.xlabel('Número de Caracteres')\n",
        "plt.ylabel('Frecuencia')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.hist(df['num_palabras'].dropna(), bins=50, alpha=0.7, color='lightblue', label='Original')\n",
        "plt.hist(df['num_palabras_procesado'].dropna(), bins=50, alpha=0.7, color='lightcoral', label='Procesado')\n",
        "plt.title('Distribución de Número de Palabras')\n",
        "plt.xlabel('Número de Palabras')\n",
        "plt.ylabel('Frecuencia')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "reduccion_por_texto = ((df['longitud_texto'] - df['longitud_procesado']) / df['longitud_texto'].replace(0, 1)) * 100\n",
        "plt.hist(reduccion_por_texto.dropna(), bins=50, alpha=0.7, color='lightgreen')\n",
        "plt.title('Reducción de Longitud por Texto')\n",
        "plt.xlabel('% de Reducción')\n",
        "plt.ylabel('Frecuencia')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "eQ5o0abIlzX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## 4. Preparación del pipeline"
      ],
      "metadata": {
        "id": "qnVtC6a8nb-P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 Separación de variable objetivo\n",
        "\n",
        "Se separa la variable objetivo de los textos. Se hará división 80/20."
      ],
      "metadata": {
        "id": "eLFwB20kowJt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# División de datos\n",
        "X = data_raw['textos']\n",
        "y = data_raw['ODS']\n",
        "\n",
        "# División estratificada para mantener proporciones de clases\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")"
      ],
      "metadata": {
        "id": "GbQTx1XkndUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2. Pipeline\n",
        "\n",
        "El pipeline se compone de varias etapas encadenadas: primero el preprocesador de texto, luego la vectorización mediante TF-IDF, seguido de un paso de densificación que se aplica únicamente cuando se usa PCA (ya que este método no trabaja con matrices dispersas). Posteriormente, se aplica el reductor de dimensionalidad (PCA o TruncatedSVD) y, finalmente, el clasificador. Se eligió Regresión Logística por ser un modelo eficiente, interpretable y ampliamente utilizado en tareas de clasificación de texto, especialmente en escenarios multiclase como en este caso."
      ],
      "metadata": {
        "id": "SNxBsU7dy2I5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Pasos del pipeline\n",
        "steps = [\n",
        "    ('preprocessor', text_preprocessor),\n",
        "    (\"tfidf\", TfidfVectorizer()),\n",
        "    (\"densify\", FunctionTransformer(lambda x: x.toarray(), accept_sparse=True)),\n",
        "    (\"reductor\", PCA(random_state=0)),\n",
        "    (\"clf\", LogisticRegression(random_state=0, max_iter=2000))\n",
        "]\n",
        "\n",
        "#Objeto de pipeline\n",
        "pipe = Pipeline(steps)"
      ],
      "metadata": {
        "id": "KIOPk8SexWe1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Búsqueda de hiperparámetros\n",
        "\n",
        "### 5.1. Definición de la grilla\n",
        "\n",
        "En esta etapa se define la grilla de hiperparámetros que será utilizada en el proceso de búsqueda con GridSearchCV. Se consideran dos posibles reductores de dimensionalidad: PCA y TruncatedSVD. Asimismo, se ajustan parámetros propios del vectorizador TF-IDF, como el rango de n-grams y el umbral máximo de frecuencia de términos (max_df). Finalmente, se incluyen variaciones en la penalización (l1 y l2) y en el hiperparámetro de regularización C de la Regresión Logística. Esta combinación de opciones permite explorar un amplio espacio de configuraciones, con el fin de identificar el modelo que ofrezca el mejor desempeño en la tarea de clasificación de textos según los ODS."
      ],
      "metadata": {
        "id": "SsdmLmSf0gWx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Definición del param_grid\n",
        "param_grid =[\n",
        "    # PCA\n",
        "    {\n",
        "        \"reductor\": [PCA(random_state=0)],\n",
        "        \"reductor__n_components\": [50, 300, 500],\n",
        "        \"tfidf__max_df\": [0.75, 1.0],\n",
        "        \"tfidf__ngram_range\": [(1,1), (1,2)],\n",
        "        \"tfidf__max_features\": [5000, 10000],\n",
        "        \"clf__C\": [0.01, 0.1, 1, 10, 100]\n",
        "    },\n",
        "\n",
        "    #TruncatedSVD\n",
        "    {\n",
        "        \"reductor\": [TruncatedSVD(random_state=0)],\n",
        "        \"reductor__n_components\": [50, 300, 500],\n",
        "        \"tfidf__max_df\": [0.75, 1.0],\n",
        "        \"tfidf__ngram_range\": [(1,1), (1,2)],\n",
        "        \"tfidf__max_features\": [5000, 10000],\n",
        "        \"densify\": [\"passthrough\"],\n",
        "        \"clf__C\": [ 0.01, 0.1, 1, 10, 100]\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "7O_OAcZZ0tN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.2. Cross-validation\n",
        "\n",
        "En esta etapa se define el esquema de validación cruzada que se utilizará durante la búsqueda de hiperparámetros. Se opta por un Stratified K-Fold con 5 particiones, el cual divide el conjunto de datos en cinco bloques manteniendo la misma proporción de clases en cada fold. Además, se activa la opción shuffle=True junto con un random_state fijo, lo que garantiza que las divisiones se realicen de manera aleatoria pero reproducible en ejecuciones posteriores. Este procedimiento permite evaluar de forma más robusta el desempeño del modelo y reduce el riesgo de sobreajuste asociado a una única partición de entrenamiento y prueba."
      ],
      "metadata": {
        "id": "Y9NbIGH63IGP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)"
      ],
      "metadata": {
        "id": "f-mNPOLt3Hzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.3. GridSearchCV\n",
        "\n",
        "En este paso se ejecuta la búsqueda exhaustiva de hiperparámetros mediante GridSearchCV, combinando el pipeline, la grilla definida y la validación cruzada de 5 folds. Se usa n_jobs=1 y verbose=2 para visualizar el progreso, con el fin de identificar la mejor configuración del modelo."
      ],
      "metadata": {
        "id": "sVjdZZn45Ej-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grid = GridSearchCV(\n",
        "    pipe,\n",
        "    param_grid,\n",
        "    cv=cv,\n",
        "    n_jobs=4,\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "#Entrena el modelo\n",
        "grid.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "jYLhMdPu4UX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.4. Mejor modelo\n"
      ],
      "metadata": {
        "id": "AFT-Sp4V5-Qf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mejor combinación de hiperparámetros\n",
        "print(\"Mejores hiperparámetros encontrados:\")\n",
        "print(grid.best_params_)\n",
        "\n",
        "# Mejor score obtenido en CV\n",
        "print(f\"Mejor score de validación: {grid.best_score_:.4f}\")\n",
        "\n",
        "# Modelo ya entrenado con los mejores parámetros\n",
        "best_model = grid.best_estimator_"
      ],
      "metadata": {
        "id": "8yy3TkKN6AXv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}